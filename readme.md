ğŸš¦ IoT-Based Smart Traffic Analytics Pipeline

End-to-End Data Engineering Project simulating IoT traffic sensors, building batch + streaming pipelines, and visualizing real-time insights on a dashboard â€” with direct database integration.

ğŸ“Œ Project Overview

Modern smart cities rely on IoT traffic sensors to monitor vehicles, detect congestion, and improve road safety.
This project simulates such a system by:

Generating realistic traffic data (vehicle ID, speed, location, timestamp).

Ingesting data directly into a database instead of flat files.

Processing it in batch ETL pipelines for historical insights.

Streaming it in real-time pipelines for live alerts.

Visualizing insights on a dashboard for monitoring.

This project demonstrates core data engineering skills:
âœ… IoT Data Simulation
âœ… Database Ingestion (SQLite â†’ Azure SQL / Data Lake)
âœ… Batch ETL with Pandas & SQL
âœ… Streaming Analytics with Kafka / Azure Stream Analytics
âœ… Real-Time Dashboards

ğŸ¯ Objectives

Simulate IoT Traffic Data â€“ Python script generates live traffic events into a database.

Batch Data Pipeline (ETL) â€“ Clean, transform & load processed data into a new table.

Streaming Analytics â€“ Real-time alerts for overspeeding, congestion, and accidents.

Dashboard & Reporting â€“ Visualize metrics and summarize findings.

ğŸ› ï¸ Tech Stack
Layer Tools & Technologies
Data Simulation Python (random, faker)
Database SQLite (local), Azure SQL Database
Batch Processing Pandas, SQL queries
Streaming Azure Stream Analytics / Apache Kafka
Storage SQL Database, Azure Data Lake
Visualization Power BI / Streamlit / Grafana
Big Data (Optional) Spark, Hadoop
Orchestration (Optional) Airflow
ğŸ“‚ Project Structure
ğŸ“¦ smart-traffic-analytics
â”£ ğŸ“œ README.md
â”£ ğŸ“œ traffic_simulator.py # Data generator â†’ Database ingestion
â”£ ğŸ“œ traffic.db # SQLite database (raw traffic data)
â”£ ğŸ“œ traffic_etl.py # Batch ETL pipeline
â”£ ğŸ“œ processed_traffic.db # Processed table (after ETL)
â”£ ğŸ“œ streaming_pipeline/ # Real-time processing setup
â”£ ğŸ“œ dashboard/ # Dashboard code (Power BI / Streamlit)
â”£ ğŸ“œ report/ # Final PDF Report

ğŸš€ Milestones
Milestone 1: Data Simulation (Database-First)

âœ… Python script simulates traffic data:

vehicle_id, speed, location, timestamp.

âœ… Data ingested directly into traffic.db (SQLite).

Sample Record:

V605 | 133 | Downtown | 2025-09-03 06:12:38
V744 | 56 | Highway A1 | 2025-09-03 06:12:43

Milestone 2: Batch ETL Pipeline

âœ… Extract: Read raw traffic data from traffic.db.

âœ… Transform:

Flag overspeeding vehicles (>120 km/h).

Compute average speeds per location.

Handle duplicates/missing values.

âœ… Load: Save processed data into new table (processed_traffic).

Milestone 3: Streaming Analytics

âœ… Send real-time events to Azure Event Hub / Kafka.

âœ… Process streams with Azure Stream Analytics.

âœ… Generate alerts for:

Overspeeding.

Sudden congestion (avg speed drop).

Accidents (vehicles stuck at 0 km/h).

Milestone 4: Dashboard & Reporting

âœ… Dashboard (Power BI / Streamlit / Grafana).

âœ… Real-time monitoring:

Vehicle speeds per road.

Live alerts.

Rush-hour congestion trends.

âœ… Final PDF Report with:

System architecture.

Key insights.

Performance results.

ğŸ“Š System Architecture
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Data Source â”‚ (Python IoT Generator â†’ Database)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
â”‚
â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Batch ETL â”‚ (Pandas + SQL)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Processed DB â”‚ (SQLite â†’ Azure SQL / Data Lake)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Streaming â”‚ (Kafka / Azure Stream Analytics)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Dashboard â”‚ (Power BI / Streamlit)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“¸ Screenshots (to add later)

âœ… Console logs of data insertion into DB.

âœ… Raw traffic_data table in SQLite.

âœ… Processed ETL table with flagged anomalies.

âœ… Dashboard with real-time metrics.

ğŸ† Final Deliverables

Python scripts (traffic_simulator.py, traffic_etl.py).

Databases (traffic.db, processed_traffic.db).

Streaming pipeline config (Azure/Kafka).

Dashboard (Power BI / Streamlit).

Final Report (PDF) documenting:

Pipeline architecture.

Key insights.

System performance.

ğŸ‘¥ Team Roles
Role Member Responsibility
Data Simulation Lead Python generator â†’ Database ingestion
Batch ETL Engineer Build ETL logic with Pandas + SQL
Streaming Engineer Kafka / Azure Stream Analytics
Cloud Architect Azure SQL, Event Hub, Data Lake setup
Dashboard Developer Power BI / Streamlit dashboards
Project Manager Integration + final report
ğŸŒŸ Key Learnings

IoT data ingestion directly into databases.

Batch analytics with Pandas & SQL.

Streaming pipelines for real-time traffic monitoring.

Building dashboards for live insights.

Deploying pipelines to cloud platforms.

ğŸ“œ License

This project is for educational purposes as part of a Data Engineering course.

## Local Kafka + Hive (developer setup)

If you want to run the pipeline against a local Kafka broker and a HiveServer2 instance (recommended for end-to-end testing), follow these steps.

1. Start Kafka (local broker)

- A ready-to-run Docker Compose for Kafka (Zookeeper + Kafka) is included: `docker-compose.kafka.yml`.
- Start it from the project root:

```powershell
docker compose -f docker-compose.kafka.yml up -d
```

- Kafka broker will be reachable at `localhost:9092`.

2. Start HiveServer2 (options)

- Hive is heavier to run locally. For development you have two practical options: - Use an existing Hive cluster (set `HIVE_HOST`/`HIVE_PORT` in the scripts to point to it). - Run a Hive dev stack in Docker. There are community-maintained Docker Compose projects that spin up Hadoop + Hive + HiveServer2 + metastore (MySQL/Postgres). A reliable example is the `bde2020` Hive images; see: https://github.com/big-data-europe/docker-hive

If you want, I can add an opinionated `docker-compose.hive.yml` (Hadoop + MySQL metastore + HiveServer2) to this repo â€” it will be heavier but will let you run HiveServer2 locally.

3. Install Python dependencies

```powershell
python -m pip install --upgrade pip
python -m pip install -r requirements.txt
```

4. Run the ETL and viewer (strict mode â€” Kafka + Hive required by default)

```powershell
# Run ETL (requires Kafka + Hive unless you pass --allow-fallback)
python .\src\etl\traffic_etl.py

# Inspect processed table using the Hive viewer
python .\src\analytics\view_data.py processed_traffic 10
```

If you prefer fallbacks (JSONL/SQLite) temporarily while bringing up Hive, add `--allow-fallback` to the commands.

If you'd like, I can add a full `docker-compose.hive.yml` and test the end-to-end run locally â€” say "Yes, add Hive compose" and I'll scaffold it next.
